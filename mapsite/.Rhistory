text(tree.carseats, pretty = 0)
yhat <- predict(tree.carseats, newdata = Carseats.test)
mean((yhat - Carseats.test$Sales)^2)
cv.carseats <- cv.tree(tree.carseats)
plot(cv.carseats$size, cv.carseats$dev, type = "b")
tree.min <- which.min(cv.carseats$dev)
points(tree.min, cv.carseats$dev[tree.min], col = "red", cex = 2, pch = 20)
yhat <- predict(prune.carseats, newdata = Carseats.test)
cv.carseats <- cv.tree(tree.carseats)
plot(cv.carseats$size, cv.carseats$dev, type = "b")
tree.min <- which.min(cv.carseats$dev)
points(tree.min, cv.carseats$dev[tree.min], col = "red", cex = 2, pch = 20)
prune.carseats <- prune.tree(tree.carseats, best = 8)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
yhat <- predict(prune.carseats, newdata = Carseats.test)
mean((yhat - Carseats.test$Sales)^2)
bag.carseats <- randomForest(Sales ~ ., data = Carseats.train, mtry = 10, ntree = 500, importance = TRUE)
yhat.bag <- predict(bag.carseats, newdata = Carseats.test)
mean((yhat.bag - Carseats.test$Sales)^2)
library("randomForest")
install.packages("randomForrest")
library("randomForrest")
install.packages("randomForest")
library("randomForest")
bag.carseats <- randomForest(Sales ~ ., data = Carseats.train, mtry = 10, ntree = 500, importance = TRUE)
yhat.bag <- predict(bag.carseats, newdata = Carseats.test)
mean((yhat.bag - Carseats.test$Sales)^2)
set.seed(1)
train <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
Carseats.train <- Carseats[train, ]
Carseats.test <- Carseats[-train, ]
tree.carseats <- tree(Sales ~ ., data = Carseats.train)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
yhat <- predict(tree.carseats, newdata = Carseats.test)
mean((yhat - Carseats.test$Sales)^2)
#Test MSE is about 4.15
cv.carseats <- cv.tree(tree.carseats)
plot(cv.carseats$size, cv.carseats$dev, type = "b")
tree.min <- which.min(cv.carseats$dev)
points(tree.min, cv.carseats$dev[tree.min], col = "red", cex = 2, pch = 20)
prune.carseats <- prune.tree(tree.carseats, best = 8)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
yhat <- predict(prune.carseats, newdata = Carseats.test)
mean((yhat - Carseats.test$Sales)^2)
#pruning tree increases Test MSE to 5.09
bag.carseats <- randomForest(Sales ~ ., data = Carseats.train, mtry = 10, ntree = 500, importance = TRUE)
yhat.bag <- predict(bag.carseats, newdata = Carseats.test)
mean((yhat.bag - Carseats.test$Sales)^2)
importance(bag.carseats)
rf.carseats <- randomForest(Sales ~ ., data = Carseats.train, mtry = 3, ntree = 500, importance = TRUE)
yhat.rf <- predict(rf.carseats, newdata = Carseats.test)
mean((yhat.rf - Carseats.test$Sales)^2)
importance(rf.carseats)
library("ISLR")
library("tree")
library("randomForest")
#create training and test set
set.seed(1)
trainset    <- sample(1:nrow(OJ), 800)
OJ_train    <- OJ[train, ]
OJ_test     <- OJ[-train, ]
OJ_train    <- OJ[trainset, ]
OJ_test     <- OJ[-trainset, ]
#fitting tree
tree_oj <- tree(Purchase ~ ., data = OJ_train)
summary(tree_oj)
#The fitted tree has 8 terminal nodes and a training error rate of 0.165
#fitting tree
tree_oj <- tree(Purchase ~ ., data = OJ_train)
summary(tree_oj)
#The fitted tree has 8 terminal nodes and a training error rate of 0.165
tree.oj
# Label 9 is a terminal node due the astricks. The split criterion is LoyalCH > 0.0356415
# Number of observations is 109 w/ deviance 100.9 and overal prediction of branch MM.
# 82.56% of predictions take MM , 17.43% take CH
plot(tree_oj)
text(tree_oj, pretty = 0)
tree_pred     <- predict(tree_oj, OJ_test, type = "class")
predTableTree <-  table(tree_pred, OJ_test$Purchase)
predTableTree
correct <- predTable[1] + predTable[4]
all <- correct + predTable[2] + predTable[3]
1 - (correct/all)
correct <- predTableTree[1] + predTableTree[4]
all <- correct + predTableTree[2] + predTableTree[3]
1 - (correct/all)
cv_oj <- cv.tree(tree_oj, FUN = prune.misclass)
cv_oj
plot(cv_oj$size, cv_oj$dev, type = "b", xlab = "Tree size", ylab = "Deviance")
prune_oj <- prune.misclass(tree.oj, best = 2)
plot(prune_oj)
text(prune_oj, pretty = 0)
summary(prune_oj)
summary(tree_oj)
prune_oj <- prune.misclass(tree_oj, best = 2)
plot(prune_oj)
text(prune_oj, pretty = 0)
summary(prune_oj)
summary(tree_oj)
prune_pred   <- predict(prune_oj, OJ_test, type = "class")
predPrunTree <-  table(prune_pred, OJ_test$Purchase)
predPrunTree
correct <- predPrunTree[1] + predPrunTree[4]
all <- correct + predPrunTree[2] + predPrunTree[3]
1 - (correct/all)
set.seed(1)
train          <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
carseats_train <- Carseats[train, ]
carseats_test  <- Carseats[-train, ]
tree_carseats  <- tree(Sales ~ ., data = Carseats.train)
summary(tree_carseats)
train          <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
carseats_train <- Carseats[train, ]
carseats_test  <- Carseats[-train, ]
tree_carseats  <- tree(Sales ~ ., data = carseats_train)
summary(tree_carseats)
plot(tree_carseats)
text(tree_carseats, pretty = 0)
yhat <- predict(tree.carseats, newdata = Carseats.test)
mean((yhat - Carseats.test$Sales)^2)
y_hat <- predict(tree_carseats, newdata = carseats_test)
mean((y_hat - carseats_test$Sales)^2)
set.seed(1)
train          <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
carseats_train <- Carseats[train, ]
carseats_test  <- Carseats[-train, ]
#fitting a regression tree
tree_carseats  <- tree(Sales ~ ., data = carseats_train)
summary(tree_carseats)
plot(tree_carseats)
text(tree_carseats, pretty = 0)
y_hat <- predict(tree_carseats, newdata = carseats_test)
mean((y_hat - carseats_test$Sales)^2)
#cross-validation
cv_carseats <- cv.tree(tree_carseats)
plot(cv_carseats$size, cv_carseats$dev, type = "b")
tree_min <- which.min(cv_carseats$dev)
points(tree_min, cv_carseats$dev[tree_min], col = "red", cex = 2, pch = 20)
prune_carseats <- prune.tree(tree_carseats, best = 8)
plot(prune_carseats)
text(prune_carseats, pretty = 0)
y_hat.prune <- predict(prune.carseats, newdata = Carseats.test)
mean((y_hat.prune - Carseats.test$Sales)^2)
#pruning tree increases Test MSE to 5.09
y_hat.prune <- predict(prune_carseats, newdata = carseats_test)
mean((y_hat.prune - Carseats.test$Sales)^2)
mean((y_hat.prune - carseats_test$Sales)^2)
bag_carseats <- randomForest(Sales ~ ., data = carseats_train, mtry = 10, ntree = 500, importance = TRUE)
y_hat.bag <- predict(bag.carseats, newdata = Carseats.test)
mean((y_hat.bag - carseats_test$Sales)^2)
importance(bag_carseats)
mean((y_hat.bag - carseats_test$Sales)^2)
y_hat.bag <- predict(bag.carseats, newdata = carseats_test)
y_hat.bag <- predict(bag_carseats, newdata = carseats_test)
mean((y_hat.bag - carseats_test$Sales)^2)
rf_carseats <- randomForest(Sales ~ ., data = carseats_train, mtry = 3, ntree = 500, importance = TRUE)
y_hat.rf <- predict(rf_carseats, newdata = carseats_test)
mean((y_hat.rf - carseats_test$Sales)^2)
importance(rf_carseats)
library("ISLR")
library("tree")
library("randomForest")
#create training and test set
set.seed(1)
trainset    <- sample(1:nrow(OJ), 800)
OJ_train    <- OJ[trainset, ]
OJ_test     <- OJ[-trainset, ]
#fitting tree
tree_oj <- tree(Purchase ~ ., data = OJ_train)
summary(tree_oj)
#The fitted tree has 8 terminal nodes and a training error rate of 0.165
tree.oj
# Label 9 is a terminal node due the astricks. The split criterion is LoyalCH > 0.0356415
# Number of observations is 109 w/ deviance 100.9 and overal prediction of branch MM.
# 82.56% of predictions take MM , 17.43% take CH
plot(tree_oj)
text(tree_oj, pretty = 0)
#We may see that the most important indicator of Purchase appears to be LoyalCH, since the first branch differentiates
#the intensity of customer brand loyalty to CH. Top 3 nodes also have LoyalCH
tree_pred     <- predict(tree_oj, OJ_test, type = "class")
predTableTree <-  table(tree_pred, OJ_test$Purchase)
predTableTree
correct <- predTableTree[1] + predTableTree[4]
all <- correct + predTableTree[2] + predTableTree[3]
1 - (correct/all)
#The test error tate is about 22.59%
#apply cv.tree
cv_oj <- cv.tree(tree_oj, FUN = prune.misclass)
cv_oj
plot(cv_oj$size, cv_oj$dev, type = "b", xlab = "Tree size", ylab = "Deviance")
#The 2 node tree is smalest tree with lowest classfication error rate
prune_oj <- prune.misclass(tree_oj, best = 2)
plot(prune_oj)
text(prune_oj, pretty = 0)
summary(prune_oj)
summary(tree_oj)
#Training error: .165(unpruned) vs .1825 (prune)
prune_pred   <- predict(prune_oj, OJ_test, type = "class")
predPrunTree <-  table(prune_pred, OJ_test$Purchase)
predPrunTree
correct <- predPrunTree[1] + predPrunTree[4]
all <- correct + predPrunTree[2] + predPrunTree[3]
1 - (correct/all)
#pruning increased the test error rate to 26.0%(orginal tree was ~22%) but pruned tree with two nodes has better interpretability(easier to intepret)
set.seed(1)
train          <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
carseats_train <- Carseats[train, ]
carseats_test  <- Carseats[-train, ]
#fitting a regression tree
tree_carseats  <- tree(Sales ~ ., data = carseats_train)
summary(tree_carseats)
plot(tree_carseats)
text(tree_carseats, pretty = 0)
y_hat.tree <- predict(tree_carseats, newdata = carseats_test)
mean((y_hat.tree - carseats_test$Sales)^2)
#Test MSE for the regression tree is about 4.15
#cross-validation
cv_carseats <- cv.tree(tree_carseats)
plot(cv_carseats$size, cv_carseats$dev, type = "b")
tree_min <- which.min(cv_carseats$dev)
points(tree_min, cv_carseats$dev[tree_min], col = "red", cex = 2, pch = 20)
prune_carseats <- prune.tree(tree_carseats, best = 8)
plot(prune_carseats)
text(prune_carseats, pretty = 0)
y_hat.prune <- predict(prune_carseats, newdata = carseats_test)
mean((y_hat.prune - carseats_test$Sales)^2)
#Pruning tree increases Test MSE to 5.09
#appying bagging
bag_carseats <- randomForest(Sales ~ ., data = carseats_train, mtry = 10, ntree = 500, importance = TRUE)
y_hat.bag <- predict(bag_carseats, newdata = carseats_test)
mean((y_hat.bag - carseats_test$Sales)^2)
importance(bag_carseats)
#Test MSE decreased to 2.60 when using bagging, the Orginal Regression was 4.15
#Price and Shelf Locaction are the two most imporatnt variabes
#applying randomForest
rf_carseats <- randomForest(Sales ~ ., data = carseats_train, mtry = 3, ntree = 500, importance = TRUE)
y_hat.rf <- predict(rf_carseats, newdata = carseats_test)
mean((y_hat.rf - carseats_test$Sales)^2)
importance(rf_carseats)
#Price and Shelve Locaction are the two most imporatnt variabes
#In this case, with m=sqrt(p), Test MSE for Random forest is 3.296
install.packages('osmar')
library(osmar)
src <- osmsource_api()
src
long <- 32.28
lat <- 36.43
bb <- center_bbox(long, lat, 1000, 1000)
ctown <- get_osm(bb, source = src)
plot(ctown)
long <- 32.28
lat <- 36.43
bb <- center_bbox(long, lat, 2000, 2000)
ctown <- get_osm(bb, source = src)
plot(ctown)
plot(ctown)
long <- 32.28
lat <- 36.43
bb <- center_bbox(long, lat, 2000, 2000)
ctown <- get_osm(bb, source = src)
plot(ctown)
long <- 32.28
lat <- 36.43
bb <- center_bbox(long, lat, 5000, 5000)
ctown <- get_osm(bb, source = src)
plot(ctown)
library(OpenStreetMap)
install.packages("OpenStreetMap")
library(OpenStreetMap)
library(rgdal)
lat <- c(32.3143,32.2630)
long <- c(36.2989,36.3597)
southwest <- openmap(c(lat[1],lon[1]),c(lat[2],lon[2]),zoom=6,'osm')
library(OpenStreetMap)
library(rgdal)
library(OpenStreetMap)
library(rgdal)
lat <- c(32.3143,32.2630)
long <- c(36.2989,36.3597)
southwest <- openmap(c(lat[1],lon[1]),c(lat[2],lon[2]),zoom=6,'osm')
lat <- c(32.3143,32.2630)
lon <- c(36.2989,36.3597)
southwest <- openmap(c(lat[1],lon[1]),c(lat[2],lon[2]),zoom=6,'osm')
plot(southwest)
library(OpenStreetMap)
library(rgdal)
lat <- c(32.3143,32.2630)
lon <- c(36.2989,36.3597)
southwest <- openmap(c(lat[1],lon[1]),c(lat[2],lon[2]),zoom=10,'osm')
plot(southwest)
library(OpenStreetMap)
library(rgdal)
lat <- c(32.3143,32.2630)
lon <- c(36.2989,36.3597)
southwest <- openmap(c(lat[1],lon[1]),c(lat[2],lon[2]),zoom=1,'osm')
plot(southwest)
library(OpenStreetMap)
library(rgdal)
lat <- c(32.3143,32.2630)
lon <- c(36.2989,36.3597)
southwest <- openmap(c(lat[1],lon[1]),c(lat[2],lon[2]),zoom=10,'osm')
plot(southwest)
southwest <- openmap(c(lat[1],lon[1]),c(lat[2],lon[2]),zoom=100,'osm')
plot(southwest)
library(OpenStreetMap)
library(rgdal)
lat <- c(32.3143,32.2630)
lon <- c(36.2989,36.3597)
southwest <- openmap(c(lat[1],lon[1]),c(lat[2],lon[2]),zoom=50,'osm')
plot(southwest)
library(OpenStreetMap)
library(rgdal)
lat <- c(32.3143,32.2630)
lon <- c(36.2989,36.3597)
southwest <- openmap(c(lat[1],lon[1]),c(lat[2],lon[2]),zoom=10,'osm')
plot(southwest)
library(OpenStreetMap)
library(rgdal)
lat <- c(32.3143,32.2630)
lon <- c(36.2989,36.3597)
southwest <- openmap(c(lat[1],lon[1]),c(lat[2],lon[2]),zoom=5,'osm')
plot(southwest)
library(OpenStreetMap)
library(rgdal)
lat <- c(32.3143,32.2630)
lon <- c(36.2989,36.3597)
southwest <- openmap(c(lat[1],lon[1]),c(lat[2],lon[2]),zoom=15,'osm')
plot(southwest)
setwd("~/Documents/Capstone/mapsite")
read.csv("data/d2.csv")
read.csv("cap_data/d2.csv")
d2 <- read.csv("cap_data/d2.csv")
d8 <- read.csv("cap_data/d8.csv")
d9 <- read.csv("cap_data/d9.csv")
d2
head(d2)
d2
d2$district <- "d2"
d2
d8$district <- "d8"
d9$district <- "d9"
d2$district <- "d2"
typeof(d2)
typeof(d3)
typeof(d8)
typeof(d9)
f <- rbind(d2, d8)
View(d2)
colname(d2)
colnames(d2)
colnames(d2)
colnames(d8)
colnames(d2)
colnames(d2) in colnames(d8)
colnames(d2)
test <- colnames(d2)
test
for (x in test){}
for (x in test){
print(x)
}
"district" in colnames(d8)
"district" %in% names(d8)
"district1" %in% names(d8)
for (x in test){
}
test <- colnames(d2)
"district" %in% names(d8)
!"district" %in% names(d8)
test <- colnames(d2)
for (x in test){
if(!(x %in% names(d8))){
print(x)
}
}
test <- colnames(d2)
for (x in test){
if((x %in% names(d8))){
print(x)
}
}
test <- colnames(d8)
for (x in test){
if((x %in% names(d2))){
print(x)
}
}
test <- colnames(d8)
for (x in test){
if(!(x %in% names(d2))){
print(x)
}
}
library(gtools)
smartbind(d2,d8)
new <- smartbind(d2,d8)
new <- smartbind(new,d9)
new
head(new)
d2 <- read.csv("cap_data/d2.csv")
d8 <- read.csv("cap_data/d8.csv")
d9 <- read.csv("cap_data/d9.csv")
d2$district <- "d2"
d8$district <- "d8"
d9$district <- "d9"
new <- smartbind(d2,d8)
new <- smartbind(new,d9)
saveRDS(new,"data.rds")
saveRDS(new,"cap_data/data.rds")
library(shiny)
library(leaflet)
r_colors <- rgb(t(col2rgb(colors()) / 255))
names(r_colors) <- colors()
ui <- fluidPage(
leafletOutput("mymap"),
p(),
actionButton("recalc", "New points")
)
server <- function(input, output, session) {
points <- eventReactive(input$recalc, {
cbind(rnorm(40) * 2 + 13, rnorm(40) + 48)
}, ignoreNULL = FALSE)
output$mymap <- renderLeaflet({
leaflet() %>%
addProviderTiles("Stamen.TonerLite",
options = providerTileOptions(noWrap = TRUE)
) %>%
addMarkers(data = points())
})
}
shinyApp(ui, server)
runApp('test.R')
points
cbind(rnorm(40) * 2 + 13, rnorm(40) + 48)
runApp('test.R')
runApp('test.R')
runApp('test.R')
df = data.frame(Lat = 1:10, Long = rnorm(10))
leaflet(df) %>% addCircles()
m <- leaflet() %>%
addTiles() %>%  # Add default OpenStreetMap map tiles
addMarkers(lng=174.768, lat=-36.852, popup="The birthplace of R")
m
allzips
setwd("~/Documents/Capstone/mapsite")
allzips
library(leaflet)
library(RColorBrewer)
library(scales)
library(lattice)
library(dplyr)
allzips
runApp()
allzips
asset <- readRDS("cap_data/data.rds")
runApp()
asset
runApp()
runApp("mapsite")
setwd("~/Documents/Capstone")
runApp("mapsite")
runApp("mapsite")
runApp("mapsite")
runApp('mapsite')
input
runApp('mapsite')
runApp('mapsite')
runApp('mapsite')
runApp('mapsite')
runApp('mapsite')
runApp('mapsite')
runApp('mapsite')
asset
asset$district
runApp('mapsite')
runApp('mapsite')
runApp('mapsite')
runApp('mapsite')
setwd("~/Documents/Capstone")
runApp('mapsite')
runApp('mapsite')
runApp('mapsite')
runApp('mapsite')
runApp('mapsite')
runApp('mapsite')
runApp('mapsite')
runApp('mapsite')
runApp('mapsite')
d2          <- read.csv("cap_data/d2.csv")
d8          <- read.csv("cap_data/d8.csv")
d9          <- read.csv("cap_data/d9.csv")
d2$district <- "d2"
d8$district <- "d8"
d9$district <- "d9"
new         <- smartbind(d2,d8)
new         <- smartbind(new,d9)
d2          <- read.csv("cap_data/d2.csv")
d8          <- read.csv("cap_data/d8.csv")
d9          <- read.csv("cap_data/d9.csv")
setwd("~/Documents/Capstone/mapsite")
d2          <- read.csv("cap_data/d2.csv")
d8          <- read.csv("cap_data/d8.csv")
d9          <- read.csv("cap_data/d9.csv")
d2
d2$collector.block_number
unique(d2$collector.block_number)
unique(d8$collector.block_number)
unique(d9$collector.block_number)
