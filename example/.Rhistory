table(predQDA2$class, Direction0910)
mean(predQDA2$class == Direction0910)
modelLDA3 <- lda(Direction ~ Lag2*Lag2, data = Weekly, subset = train)
predLDA3 <- predict(modelLDA3, Weekly0910)
mean(predLDA3$class == Direction0910)
table(predLDA3$class, Direction0910)
modelLDA3 <- lda(Direction ~ Lag2+Lag2, data = Weekly, subset = train)
predLDA3 <- predict(modelLDA3, Weekly0910)
mean(predLDA3$class == Direction0910)
table(predLDA3$class, Direction0910)
modelLDA3 <- lda(Direction ~ Lag2+Lag1, data = Weekly, subset = train)
predLDA3 <- predict(modelLDA3, Weekly0910)
mean(predLDA3$class == Direction0910)
table(predLDA3$class, Direction0910)
modelLDA3 <- lda(Direction ~ Year, data = Weekly, subset = train)
predLDA3 <- predict(modelLDA3, Weekly0910)
mean(predLDA3$class == Direction0910)
table(predLDA3$class, Direction0910)
modelLDA3 <- lda(Direction ~ Volume*Year, data = Weekly, subset = train)
predLDA3 <- predict(modelLDA3, Weekly0910)
mean(predLDA3$class == Direction0910)
table(predLDA3$class, Direction0910)
modelLDA3 <- lda(Direction ~ Volume*Volumne, data = Weekly, subset = train)
predLDA3 <- predict(modelLDA3, Weekly0910)
mean(predLDA3$class == Direction0910)
table(predLDA3$class, Direction0910)
modelLDA3 <- lda(Direction ~ Volume*Volume, data = Weekly, subset = train)
predLDA3 <- predict(modelLDA3, Weekly0910)
mean(predLDA3$class == Direction0910)
table(predLDA3$class, Direction0910)
modelLDA3 <- lda(Direction ~ Volume, data = Weekly, subset = train)
predLDA3 <- predict(modelLDA3, Weekly0910)
mean(predLDA3$class == Direction0910)
table(predLDA3$class, Direction0910)
modelLDA2 <- lda(Direction ~ Lag2:Volume, data = Weekly, subset = train)
predLDA2 <- predict(modelLDA2, Weekly0910)
mean(predLDA2$class == Direction0910)
table(predLDA2$class, Direction0910)
modelLDA2 <- lda(Direction ~ Lag2:Volume, data = Weekly, subset = train)
predLDA2 <- predict(modelLDA2, Weekly0910)
mean(predLDA2$class == Direction0910)
table(predLDA2$class, Direction0910)
modelLDA3 <- lda(Direction ~ Volume+Lag2, data = Weekly, subset = train)
predLDA3 <- predict(modelLDA3, Weekly0910)
mean(predLDA3$class == Direction0910)
table(predLDA3$class, Direction0910)
modelQDA2 <- qda(Direction ~ poly(Lag2,2), data = Weekly, subset = train)
predQDA2 <- predict(modelQDA2, Weekly0910)
table(predQDA2$class, Direction0910)
mean(predQDA2$class == Direction0910)
modelQDA2 <- qda(Direction ~ poly(Lag2,2), data = Weekly, subset = train)
predQDA2 <- predict(modelQDA2, Weekly0910)
table(predQDA2$class, Direction0910)
mean(predQDA2$class == Direction0910)
modelQDA2 <- qda(Direction ~ poly(Lag2,2), data = Weekly, subset = train)
predQDA2 <- predict(modelQDA2, Weekly0910)
table(predQDA2$class, Direction0910)
mean(predQDA2$class == Direction0910)
modelQDA2 <- qda(Direction ~ poly(Lag2,2), data = Weekly, subset = train)
predQDA2 <- predict(modelQDA2, Weekly0910)
table(predQDA2$class, Direction0910)
mean(predQDA2$class == Direction0910)
modelQDA2 <- qda(Direction ~ poly(Lag2,2), data = Weekly, subset = train)
predQDA2 <- predict(modelQDA2, Weekly0910)
table(predQDA2$class, Direction0910)
mean(predQDA2$class == Direction0910)
modelQDA2 <- qda(Direction ~ poly(Lag2,10), data = Weekly, subset = train)
predQDA2 <- predict(modelQDA2, Weekly0910)
table(predQDA2$class, Direction0910)
mean(predQDA2$class == Direction0910)
modelQDA2 <- qda(Direction ~ poly(Lag2,4), data = Weekly, subset = train)
predQDA2 <- predict(modelQDA2, Weekly0910)
table(predQDA2$class, Direction0910)
mean(predQDA2$class == Direction0910)
source('~/Documents/KDD/Lab3/lab3.R')
source('~/Documents/KDD/Lab3/lab3.R')
mean(predQDA3$class == Direction0910)
mean(predLDA2$class == Direction0910)
source('~/Documents/KDD/Lab3/lab3.R')
mean(predQDA3$class == Direction0910)
modelGlm4   <- glm(Direction ~ Lag2:Volume, data = Weekly, family = binomial, subset = train)
mean(predGlm4$class == Direction0910)
predGlm4 <- predict(modelGlm4, Weekly0910)
mean(predGlm4$class == Direction0910)
predGlm4 <- predict(modelGlm4, Weekly0910)
mean(predGlm4$class == Direction0910)
modelGlm4   <- glm(Direction ~ Lag2:Volume, data = Weekly, family = binomial, subset = train)
predGlm4 <- predict(modelGlm4, Weekly0910)
mean(predGlm4$class == Direction0910)
table(predGlm4$class, Direction0910)
modelGlm4   <- glm(Direction ~ Lag2:Volume, data = Weekly, family = binomial, subset = train)
predGlm4 <- predict(modelGlm4, Weekly0910, type = "response")
mean(predGlm4$class == Direction0910)
predGlm4$class
predGlm4
accaurcy(modelGlm4, Weekly0910, Direction0910)
library("ISLR")
set.seed(1)
train <- sample(1:nrow(OJ), 800)
OJ.train <- OJ[train, ]
OJ.test <- OJ[-train, ]
ree.oj <- tree(Purchase ~ ., data = OJ.train)
summary(tree.oj)
tree.oj <- tree(Purchase ~ ., data = OJ.train)
tree.oj <- tree(Purchase ~ ., data = OJ.train)
install.packages("tree")
tree.oj <- tree(Purchase ~ ., data = OJ.train)
install.packages("tree")
library("ISLR")
set.seed(1)
train <- sample(1:nrow(OJ), 800)
OJ.train <- OJ[train, ]
OJ.test <- OJ[-train, ]
tree.oj <- tree(Purchase ~ ., data = OJ.train)
library("tree")
tree.oj <- tree(Purchase ~ ., data = OJ.train)
summary(tree.oj)
tree.oj
plot(tree.oj)
text(tree.oj, pretty = 0)
tree.pred <- predict(tree.oj, OJ.test, type = "class")
table(tree.pred, OJ.test$Purchase)
tree.pred <- predict(tree.oj, OJ.test, type = "class")
predTable <-  table(tree.pred, OJ.test$Purchase)
predTable
predTable[0]
predTable[0][0]
predTable[0][1]
predTable[0][2]
predTable[1]
predTable[2]
predTable[4]
predTable[4] predTable[4]
correct <- predTable[1] + predTable[4]
all <- correct + predTable[2] + predTable[3]
correct/all
(1-correct)/all
correct
correct/all
1 - (correct/all)
cv.oj <- cv.tree(tree.oj, FUN = prune.misclass)
cv.oj
plot(cv.oj$size, cv.oj$dev, type = "b", xlab = "Tree size", ylab = "Deviance")
prune.oj <- prune.misclass(tree.oj, best = 2)
prune.oj <- prune.misclass(tree.oj, best = 1)
prune.oj
prune.oj <- prune.misclass(tree.oj, best = 2)
plot(prune.oj)
text(prune.oj, pretty = 0)
summary(tree.oj)
summary(prune.oj)
prune.pred <- predict(prune.oj, OJ.test, type = "class")
predTable1 <-  table(prune.pred, OJ.test$Purchase)
predTable1
correct <- predTable1[1] + predTable1[4]
all <- correct + predTable1[2] + predTable1[3]
1 - (correct/all)
train <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
Carseats.train <- Carseats[train, ]
Carseats.test <- Carseats[-train, ]
tree.carseats <- tree(Sales ~ ., data = Carseats.train)
summary(tree.carseats)
set.seed(1)
train <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
Carseats.train <- Carseats[train, ]
Carseats.test <- Carseats[-train, ]
tree.carseats <- tree(Sales ~ ., data = Carseats.train)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
yhat <- predict(tree.carseats, newdata = Carseats.test)
mean((yhat - Carseats.test$Sales)^2)
cv.carseats <- cv.tree(tree.carseats)
plot(cv.carseats$size, cv.carseats$dev, type = "b")
tree.min <- which.min(cv.carseats$dev)
points(tree.min, cv.carseats$dev[tree.min], col = "red", cex = 2, pch = 20)
yhat <- predict(prune.carseats, newdata = Carseats.test)
cv.carseats <- cv.tree(tree.carseats)
plot(cv.carseats$size, cv.carseats$dev, type = "b")
tree.min <- which.min(cv.carseats$dev)
points(tree.min, cv.carseats$dev[tree.min], col = "red", cex = 2, pch = 20)
prune.carseats <- prune.tree(tree.carseats, best = 8)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
yhat <- predict(prune.carseats, newdata = Carseats.test)
mean((yhat - Carseats.test$Sales)^2)
bag.carseats <- randomForest(Sales ~ ., data = Carseats.train, mtry = 10, ntree = 500, importance = TRUE)
yhat.bag <- predict(bag.carseats, newdata = Carseats.test)
mean((yhat.bag - Carseats.test$Sales)^2)
library("randomForest")
install.packages("randomForrest")
library("randomForrest")
install.packages("randomForest")
library("randomForest")
bag.carseats <- randomForest(Sales ~ ., data = Carseats.train, mtry = 10, ntree = 500, importance = TRUE)
yhat.bag <- predict(bag.carseats, newdata = Carseats.test)
mean((yhat.bag - Carseats.test$Sales)^2)
set.seed(1)
train <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
Carseats.train <- Carseats[train, ]
Carseats.test <- Carseats[-train, ]
tree.carseats <- tree(Sales ~ ., data = Carseats.train)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
yhat <- predict(tree.carseats, newdata = Carseats.test)
mean((yhat - Carseats.test$Sales)^2)
#Test MSE is about 4.15
cv.carseats <- cv.tree(tree.carseats)
plot(cv.carseats$size, cv.carseats$dev, type = "b")
tree.min <- which.min(cv.carseats$dev)
points(tree.min, cv.carseats$dev[tree.min], col = "red", cex = 2, pch = 20)
prune.carseats <- prune.tree(tree.carseats, best = 8)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
yhat <- predict(prune.carseats, newdata = Carseats.test)
mean((yhat - Carseats.test$Sales)^2)
#pruning tree increases Test MSE to 5.09
bag.carseats <- randomForest(Sales ~ ., data = Carseats.train, mtry = 10, ntree = 500, importance = TRUE)
yhat.bag <- predict(bag.carseats, newdata = Carseats.test)
mean((yhat.bag - Carseats.test$Sales)^2)
importance(bag.carseats)
rf.carseats <- randomForest(Sales ~ ., data = Carseats.train, mtry = 3, ntree = 500, importance = TRUE)
yhat.rf <- predict(rf.carseats, newdata = Carseats.test)
mean((yhat.rf - Carseats.test$Sales)^2)
importance(rf.carseats)
library("ISLR")
library("tree")
library("randomForest")
#create training and test set
set.seed(1)
trainset    <- sample(1:nrow(OJ), 800)
OJ_train    <- OJ[train, ]
OJ_test     <- OJ[-train, ]
OJ_train    <- OJ[trainset, ]
OJ_test     <- OJ[-trainset, ]
#fitting tree
tree_oj <- tree(Purchase ~ ., data = OJ_train)
summary(tree_oj)
#The fitted tree has 8 terminal nodes and a training error rate of 0.165
#fitting tree
tree_oj <- tree(Purchase ~ ., data = OJ_train)
summary(tree_oj)
#The fitted tree has 8 terminal nodes and a training error rate of 0.165
tree.oj
# Label 9 is a terminal node due the astricks. The split criterion is LoyalCH > 0.0356415
# Number of observations is 109 w/ deviance 100.9 and overal prediction of branch MM.
# 82.56% of predictions take MM , 17.43% take CH
plot(tree_oj)
text(tree_oj, pretty = 0)
tree_pred     <- predict(tree_oj, OJ_test, type = "class")
predTableTree <-  table(tree_pred, OJ_test$Purchase)
predTableTree
correct <- predTable[1] + predTable[4]
all <- correct + predTable[2] + predTable[3]
1 - (correct/all)
correct <- predTableTree[1] + predTableTree[4]
all <- correct + predTableTree[2] + predTableTree[3]
1 - (correct/all)
cv_oj <- cv.tree(tree_oj, FUN = prune.misclass)
cv_oj
plot(cv_oj$size, cv_oj$dev, type = "b", xlab = "Tree size", ylab = "Deviance")
prune_oj <- prune.misclass(tree.oj, best = 2)
plot(prune_oj)
text(prune_oj, pretty = 0)
summary(prune_oj)
summary(tree_oj)
prune_oj <- prune.misclass(tree_oj, best = 2)
plot(prune_oj)
text(prune_oj, pretty = 0)
summary(prune_oj)
summary(tree_oj)
prune_pred   <- predict(prune_oj, OJ_test, type = "class")
predPrunTree <-  table(prune_pred, OJ_test$Purchase)
predPrunTree
correct <- predPrunTree[1] + predPrunTree[4]
all <- correct + predPrunTree[2] + predPrunTree[3]
1 - (correct/all)
set.seed(1)
train          <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
carseats_train <- Carseats[train, ]
carseats_test  <- Carseats[-train, ]
tree_carseats  <- tree(Sales ~ ., data = Carseats.train)
summary(tree_carseats)
train          <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
carseats_train <- Carseats[train, ]
carseats_test  <- Carseats[-train, ]
tree_carseats  <- tree(Sales ~ ., data = carseats_train)
summary(tree_carseats)
plot(tree_carseats)
text(tree_carseats, pretty = 0)
yhat <- predict(tree.carseats, newdata = Carseats.test)
mean((yhat - Carseats.test$Sales)^2)
y_hat <- predict(tree_carseats, newdata = carseats_test)
mean((y_hat - carseats_test$Sales)^2)
set.seed(1)
train          <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
carseats_train <- Carseats[train, ]
carseats_test  <- Carseats[-train, ]
#fitting a regression tree
tree_carseats  <- tree(Sales ~ ., data = carseats_train)
summary(tree_carseats)
plot(tree_carseats)
text(tree_carseats, pretty = 0)
y_hat <- predict(tree_carseats, newdata = carseats_test)
mean((y_hat - carseats_test$Sales)^2)
#cross-validation
cv_carseats <- cv.tree(tree_carseats)
plot(cv_carseats$size, cv_carseats$dev, type = "b")
tree_min <- which.min(cv_carseats$dev)
points(tree_min, cv_carseats$dev[tree_min], col = "red", cex = 2, pch = 20)
prune_carseats <- prune.tree(tree_carseats, best = 8)
plot(prune_carseats)
text(prune_carseats, pretty = 0)
y_hat.prune <- predict(prune.carseats, newdata = Carseats.test)
mean((y_hat.prune - Carseats.test$Sales)^2)
#pruning tree increases Test MSE to 5.09
y_hat.prune <- predict(prune_carseats, newdata = carseats_test)
mean((y_hat.prune - Carseats.test$Sales)^2)
mean((y_hat.prune - carseats_test$Sales)^2)
bag_carseats <- randomForest(Sales ~ ., data = carseats_train, mtry = 10, ntree = 500, importance = TRUE)
y_hat.bag <- predict(bag.carseats, newdata = Carseats.test)
mean((y_hat.bag - carseats_test$Sales)^2)
importance(bag_carseats)
mean((y_hat.bag - carseats_test$Sales)^2)
y_hat.bag <- predict(bag.carseats, newdata = carseats_test)
y_hat.bag <- predict(bag_carseats, newdata = carseats_test)
mean((y_hat.bag - carseats_test$Sales)^2)
rf_carseats <- randomForest(Sales ~ ., data = carseats_train, mtry = 3, ntree = 500, importance = TRUE)
y_hat.rf <- predict(rf_carseats, newdata = carseats_test)
mean((y_hat.rf - carseats_test$Sales)^2)
importance(rf_carseats)
library("ISLR")
library("tree")
library("randomForest")
#create training and test set
set.seed(1)
trainset    <- sample(1:nrow(OJ), 800)
OJ_train    <- OJ[trainset, ]
OJ_test     <- OJ[-trainset, ]
#fitting tree
tree_oj <- tree(Purchase ~ ., data = OJ_train)
summary(tree_oj)
#The fitted tree has 8 terminal nodes and a training error rate of 0.165
tree.oj
# Label 9 is a terminal node due the astricks. The split criterion is LoyalCH > 0.0356415
# Number of observations is 109 w/ deviance 100.9 and overal prediction of branch MM.
# 82.56% of predictions take MM , 17.43% take CH
plot(tree_oj)
text(tree_oj, pretty = 0)
#We may see that the most important indicator of Purchase appears to be LoyalCH, since the first branch differentiates
#the intensity of customer brand loyalty to CH. Top 3 nodes also have LoyalCH
tree_pred     <- predict(tree_oj, OJ_test, type = "class")
predTableTree <-  table(tree_pred, OJ_test$Purchase)
predTableTree
correct <- predTableTree[1] + predTableTree[4]
all <- correct + predTableTree[2] + predTableTree[3]
1 - (correct/all)
#The test error tate is about 22.59%
#apply cv.tree
cv_oj <- cv.tree(tree_oj, FUN = prune.misclass)
cv_oj
plot(cv_oj$size, cv_oj$dev, type = "b", xlab = "Tree size", ylab = "Deviance")
#The 2 node tree is smalest tree with lowest classfication error rate
prune_oj <- prune.misclass(tree_oj, best = 2)
plot(prune_oj)
text(prune_oj, pretty = 0)
summary(prune_oj)
summary(tree_oj)
#Training error: .165(unpruned) vs .1825 (prune)
prune_pred   <- predict(prune_oj, OJ_test, type = "class")
predPrunTree <-  table(prune_pred, OJ_test$Purchase)
predPrunTree
correct <- predPrunTree[1] + predPrunTree[4]
all <- correct + predPrunTree[2] + predPrunTree[3]
1 - (correct/all)
#pruning increased the test error rate to 26.0%(orginal tree was ~22%) but pruned tree with two nodes has better interpretability(easier to intepret)
set.seed(1)
train          <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
carseats_train <- Carseats[train, ]
carseats_test  <- Carseats[-train, ]
#fitting a regression tree
tree_carseats  <- tree(Sales ~ ., data = carseats_train)
summary(tree_carseats)
plot(tree_carseats)
text(tree_carseats, pretty = 0)
y_hat.tree <- predict(tree_carseats, newdata = carseats_test)
mean((y_hat.tree - carseats_test$Sales)^2)
#Test MSE for the regression tree is about 4.15
#cross-validation
cv_carseats <- cv.tree(tree_carseats)
plot(cv_carseats$size, cv_carseats$dev, type = "b")
tree_min <- which.min(cv_carseats$dev)
points(tree_min, cv_carseats$dev[tree_min], col = "red", cex = 2, pch = 20)
prune_carseats <- prune.tree(tree_carseats, best = 8)
plot(prune_carseats)
text(prune_carseats, pretty = 0)
y_hat.prune <- predict(prune_carseats, newdata = carseats_test)
mean((y_hat.prune - carseats_test$Sales)^2)
#Pruning tree increases Test MSE to 5.09
#appying bagging
bag_carseats <- randomForest(Sales ~ ., data = carseats_train, mtry = 10, ntree = 500, importance = TRUE)
y_hat.bag <- predict(bag_carseats, newdata = carseats_test)
mean((y_hat.bag - carseats_test$Sales)^2)
importance(bag_carseats)
#Test MSE decreased to 2.60 when using bagging, the Orginal Regression was 4.15
#Price and Shelf Locaction are the two most imporatnt variabes
#applying randomForest
rf_carseats <- randomForest(Sales ~ ., data = carseats_train, mtry = 3, ntree = 500, importance = TRUE)
y_hat.rf <- predict(rf_carseats, newdata = carseats_test)
mean((y_hat.rf - carseats_test$Sales)^2)
importance(rf_carseats)
#Price and Shelve Locaction are the two most imporatnt variabes
#In this case, with m=sqrt(p), Test MSE for Random forest is 3.296
install.packages('osmar')
library(osmar)
src <- osmsource_api()
src
long <- 32.28
lat <- 36.43
bb <- center_bbox(long, lat, 1000, 1000)
ctown <- get_osm(bb, source = src)
plot(ctown)
long <- 32.28
lat <- 36.43
bb <- center_bbox(long, lat, 2000, 2000)
ctown <- get_osm(bb, source = src)
plot(ctown)
plot(ctown)
long <- 32.28
lat <- 36.43
bb <- center_bbox(long, lat, 2000, 2000)
ctown <- get_osm(bb, source = src)
plot(ctown)
long <- 32.28
lat <- 36.43
bb <- center_bbox(long, lat, 5000, 5000)
ctown <- get_osm(bb, source = src)
plot(ctown)
library(OpenStreetMap)
install.packages("OpenStreetMap")
library(OpenStreetMap)
library(rgdal)
lat <- c(32.3143,32.2630)
long <- c(36.2989,36.3597)
southwest <- openmap(c(lat[1],lon[1]),c(lat[2],lon[2]),zoom=6,'osm')
library(OpenStreetMap)
library(rgdal)
library(OpenStreetMap)
library(rgdal)
lat <- c(32.3143,32.2630)
long <- c(36.2989,36.3597)
southwest <- openmap(c(lat[1],lon[1]),c(lat[2],lon[2]),zoom=6,'osm')
lat <- c(32.3143,32.2630)
lon <- c(36.2989,36.3597)
southwest <- openmap(c(lat[1],lon[1]),c(lat[2],lon[2]),zoom=6,'osm')
plot(southwest)
library(OpenStreetMap)
library(rgdal)
lat <- c(32.3143,32.2630)
lon <- c(36.2989,36.3597)
southwest <- openmap(c(lat[1],lon[1]),c(lat[2],lon[2]),zoom=10,'osm')
plot(southwest)
library(OpenStreetMap)
library(rgdal)
lat <- c(32.3143,32.2630)
lon <- c(36.2989,36.3597)
southwest <- openmap(c(lat[1],lon[1]),c(lat[2],lon[2]),zoom=1,'osm')
plot(southwest)
library(OpenStreetMap)
library(rgdal)
lat <- c(32.3143,32.2630)
lon <- c(36.2989,36.3597)
southwest <- openmap(c(lat[1],lon[1]),c(lat[2],lon[2]),zoom=10,'osm')
plot(southwest)
southwest <- openmap(c(lat[1],lon[1]),c(lat[2],lon[2]),zoom=100,'osm')
plot(southwest)
library(OpenStreetMap)
library(rgdal)
lat <- c(32.3143,32.2630)
lon <- c(36.2989,36.3597)
southwest <- openmap(c(lat[1],lon[1]),c(lat[2],lon[2]),zoom=50,'osm')
plot(southwest)
library(OpenStreetMap)
library(rgdal)
lat <- c(32.3143,32.2630)
lon <- c(36.2989,36.3597)
southwest <- openmap(c(lat[1],lon[1]),c(lat[2],lon[2]),zoom=10,'osm')
plot(southwest)
library(OpenStreetMap)
library(rgdal)
lat <- c(32.3143,32.2630)
lon <- c(36.2989,36.3597)
southwest <- openmap(c(lat[1],lon[1]),c(lat[2],lon[2]),zoom=5,'osm')
plot(southwest)
library(OpenStreetMap)
library(rgdal)
lat <- c(32.3143,32.2630)
lon <- c(36.2989,36.3597)
southwest <- openmap(c(lat[1],lon[1]),c(lat[2],lon[2]),zoom=15,'osm')
plot(southwest)
setwd("~/Documents/Capstone")
library(shiny)
runApp("example")
setwd("~/Documents/Capstone/example")
library(dplyr)
allzips <- readRDS("data/superzip.rds")
allzips
allzips
allzips.head
head(allzips)
tail(allzips)
